{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b6033c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-06-07 03:44:36.487838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 03:44:39.188300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10398 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:db:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import erf\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Masking, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "initial_lr = 0.001  # Starting learning rate, adjust as needed\n",
    "optimizer = Adam(learning_rate=initial_lr)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a076c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class_info = pd.read_csv('sim6/simulated_flares.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d20ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>identifier</th>\n",
       "      <th>peak_count</th>\n",
       "      <th>flare_class</th>\n",
       "      <th>background_slope</th>\n",
       "      <th>background_intercept</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>mu</th>\n",
       "      <th>sigma</th>\n",
       "      <th>tau</th>\n",
       "      <th>sigma-by-tau</th>\n",
       "      <th>snr</th>\n",
       "      <th>flare_type</th>\n",
       "      <th>discrepancy_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>831.436623</td>\n",
       "      <td>B8</td>\n",
       "      <td>-0.093638</td>\n",
       "      <td>263.669853</td>\n",
       "      <td>885.862821</td>\n",
       "      <td>638.830933</td>\n",
       "      <td>399.542396</td>\n",
       "      <td>159.060693</td>\n",
       "      <td>2.511886</td>\n",
       "      <td>24299.049873</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2091.737582</td>\n",
       "      <td>C2</td>\n",
       "      <td>0.024822</td>\n",
       "      <td>58.456700</td>\n",
       "      <td>2166.071255</td>\n",
       "      <td>470.563552</td>\n",
       "      <td>125.833680</td>\n",
       "      <td>35.464750</td>\n",
       "      <td>3.548134</td>\n",
       "      <td>18651.198919</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3922.161294</td>\n",
       "      <td>C3</td>\n",
       "      <td>0.023493</td>\n",
       "      <td>290.219479</td>\n",
       "      <td>4035.052241</td>\n",
       "      <td>395.667497</td>\n",
       "      <td>483.061773</td>\n",
       "      <td>121.339631</td>\n",
       "      <td>3.981072</td>\n",
       "      <td>53205.277596</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2569.844834</td>\n",
       "      <td>C2</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>55.317646</td>\n",
       "      <td>2594.630693</td>\n",
       "      <td>620.122604</td>\n",
       "      <td>417.667746</td>\n",
       "      <td>58.997137</td>\n",
       "      <td>7.079458</td>\n",
       "      <td>55159.632668</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1511.007118</td>\n",
       "      <td>C1</td>\n",
       "      <td>0.046458</td>\n",
       "      <td>186.910159</td>\n",
       "      <td>1554.501642</td>\n",
       "      <td>665.798403</td>\n",
       "      <td>195.766885</td>\n",
       "      <td>49.174418</td>\n",
       "      <td>3.981072</td>\n",
       "      <td>20491.219874</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>8719</td>\n",
       "      <td>9994</td>\n",
       "      <td>340.013805</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.132112</td>\n",
       "      <td>23.843661</td>\n",
       "      <td>539.811854</td>\n",
       "      <td>756.865558</td>\n",
       "      <td>370.150265</td>\n",
       "      <td>658.230596</td>\n",
       "      <td>0.562341</td>\n",
       "      <td>16935.779291</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>8720</td>\n",
       "      <td>9995</td>\n",
       "      <td>1311.381430</td>\n",
       "      <td>C1</td>\n",
       "      <td>-0.129941</td>\n",
       "      <td>293.690254</td>\n",
       "      <td>2645.833267</td>\n",
       "      <td>705.799922</td>\n",
       "      <td>434.281758</td>\n",
       "      <td>1223.972294</td>\n",
       "      <td>0.354813</td>\n",
       "      <td>49609.978883</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>8721</td>\n",
       "      <td>9997</td>\n",
       "      <td>633.569737</td>\n",
       "      <td>B6</td>\n",
       "      <td>-0.008776</td>\n",
       "      <td>25.868263</td>\n",
       "      <td>1278.287106</td>\n",
       "      <td>827.993831</td>\n",
       "      <td>69.064422</td>\n",
       "      <td>194.649988</td>\n",
       "      <td>0.354813</td>\n",
       "      <td>12308.233653</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8722</th>\n",
       "      <td>8722</td>\n",
       "      <td>9998</td>\n",
       "      <td>212.221647</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.052757</td>\n",
       "      <td>134.921556</td>\n",
       "      <td>2357.524871</td>\n",
       "      <td>500.254710</td>\n",
       "      <td>2.892275</td>\n",
       "      <td>72.650656</td>\n",
       "      <td>0.039811</td>\n",
       "      <td>817.381937</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>8723</td>\n",
       "      <td>9999</td>\n",
       "      <td>448.191806</td>\n",
       "      <td>B4</td>\n",
       "      <td>-0.069297</td>\n",
       "      <td>785.307186</td>\n",
       "      <td>676.130531</td>\n",
       "      <td>839.286409</td>\n",
       "      <td>192.181433</td>\n",
       "      <td>304.587045</td>\n",
       "      <td>0.630957</td>\n",
       "      <td>7576.457249</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8724 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  identifier   peak_count flare_class  background_slope  \\\n",
       "0              0           1   831.436623          B8         -0.093638   \n",
       "1              1           2  2091.737582          C2          0.024822   \n",
       "2              2           3  3922.161294          C3          0.023493   \n",
       "3              3           4  2569.844834          C2          0.106797   \n",
       "4              4           5  1511.007118          C1          0.046458   \n",
       "...          ...         ...          ...         ...               ...   \n",
       "8719        8719        9994   340.013805          B3          0.132112   \n",
       "8720        8720        9995  1311.381430          C1         -0.129941   \n",
       "8721        8721        9997   633.569737          B6         -0.008776   \n",
       "8722        8722        9998   212.221647          B2          0.052757   \n",
       "8723        8723        9999   448.191806          B4         -0.069297   \n",
       "\n",
       "      background_intercept    amplitude          mu       sigma          tau  \\\n",
       "0               263.669853   885.862821  638.830933  399.542396   159.060693   \n",
       "1                58.456700  2166.071255  470.563552  125.833680    35.464750   \n",
       "2               290.219479  4035.052241  395.667497  483.061773   121.339631   \n",
       "3                55.317646  2594.630693  620.122604  417.667746    58.997137   \n",
       "4               186.910159  1554.501642  665.798403  195.766885    49.174418   \n",
       "...                    ...          ...         ...         ...          ...   \n",
       "8719             23.843661   539.811854  756.865558  370.150265   658.230596   \n",
       "8720            293.690254  2645.833267  705.799922  434.281758  1223.972294   \n",
       "8721             25.868263  1278.287106  827.993831   69.064422   194.649988   \n",
       "8722            134.921556  2357.524871  500.254710    2.892275    72.650656   \n",
       "8723            785.307186   676.130531  839.286409  192.181433   304.587045   \n",
       "\n",
       "      sigma-by-tau           snr flare_type  discrepancy_flag  \n",
       "0         2.511886  24299.049873          B                 0  \n",
       "1         3.548134  18651.198919          A                 0  \n",
       "2         3.981072  53205.277596          A                 1  \n",
       "3         7.079458  55159.632668          A                 0  \n",
       "4         3.981072  20491.219874          A                 0  \n",
       "...            ...           ...        ...               ...  \n",
       "8719      0.562341  16935.779291          B                 0  \n",
       "8720      0.354813  49609.978883          B                 0  \n",
       "8721      0.354813  12308.233653          B                 0  \n",
       "8722      0.039811    817.381937          B                 0  \n",
       "8723      0.630957   7576.457249          B                 0  \n",
       "\n",
       "[8724 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468091d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see the SNR distribution\n",
    "plt.hist(class_info['snr'], bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b86d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see peak count distribution\n",
    "plt.hist(class_info['peak_count'], bins = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c766e0",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e604501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8724/8724 [00:19<00:00, 458.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "data_deriv = []\n",
    "labels = []\n",
    "ids = []\n",
    "peak_count = []\n",
    "\n",
    "tot_flares = 0\n",
    "maxlen = 0\n",
    "\n",
    "for i, iden in enumerate(tqdm(np.array(class_info['identifier']))):\n",
    "    #print(iden)\n",
    "    try:\n",
    "        tod = pd.read_csv('sim6/flares/' + f'{i}' + '.csv')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "    if len(tod['Counts']) > maxlen:\n",
    "        maxlen = len(tod['Counts'])\n",
    "\n",
    "    tot_flares += 1\n",
    "    data.append(list(tod['Counts']))\n",
    "    data_deriv.append(list(np.diff(tod['Counts'])))\n",
    "    peak_count.append(class_info['peak_count'][i])\n",
    "    \n",
    "    if class_info['flare_type'][i] == 'A':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "    ids.append(class_info['identifier'][i])\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(tot_flares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b590632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [i for i in class_info['identifier']]\n",
    "# peak_count = [int(count) for count in class_info['peak_count']]\n",
    "\n",
    "# data = []\n",
    "# data_deriv = []\n",
    "# labels = []\n",
    "\n",
    "# for i in ids:\n",
    "#     tod = pd.read_csv('sim6/flares/' + f'{i}' + '.csv')\n",
    "    \n",
    "#     data.append(list(tod['Counts']))\n",
    "#     data_deriv.append(list(np.diff(tod['Counts'])))\n",
    "    \n",
    "# for index in range(len(class_info)):\n",
    "#     if str(class_info['flare_type'][index]) == 'A':\n",
    "#         labels.append(0)\n",
    "#     if str(class_info['flare_type'][index]) == 'B':\n",
    "#         labels.append(1)\n",
    "#     else:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec817a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(data, padding='post', dtype=float, maxlen=maxlen)   # the padding is to adjust the shape of the np arrays\n",
    "data_deriv = pad_sequences(data_deriv, padding='post', dtype=float, maxlen=maxlen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf54c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7615, 1999)\n",
      "(7615, 1999)\n",
      "(7615,)\n",
      "(7615,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data_deriv.shape)\n",
    "print(np.array(labels).shape)\n",
    "print(np.array(ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df044c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 1999)\n",
      "(381, 1999)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "\n",
    "for train_ind, val_ind in kf.split(data):\n",
    "    data_train, data_val = data[train_ind], data[val_ind]\n",
    "    labels_train, labels_val = np.array(labels)[train_ind], np.array(labels)[val_ind]\n",
    "    ids_train, ids_val = np.array(ids)[train_ind], np.array(ids)[val_ind]\n",
    "    data_deriv_train, data_deriv_val = data_deriv[train_ind], data_deriv[val_ind]\n",
    "    \n",
    "    break\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12351466",
   "metadata": {},
   "source": [
    "# defining model 1 and model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a625015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 1999, 1)           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1997, 64)          256       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 998, 64)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 998, 64)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 996, 128)          24704     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 498, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 498, 128)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 496, 256)          98560     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 248, 256)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 63488)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8126592   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8250241 (31.47 MB)\n",
      "Trainable params: 8250241 (31.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Input(shape=(data.shape[1], 1)))\n",
    "\n",
    "model1.add(Masking(mask_value=0.0))\n",
    "    \n",
    "# First convolutional layer\n",
    "model1.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "# Second convolutional layer\n",
    "model1.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "# Third convolutional layer\n",
    "model1.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a3ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_1 (Masking)         (None, 1999, 1)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1997, 64)          256       \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 998, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 998, 64)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 996, 128)          24704     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 498, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 498, 128)          0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 496, 256)          98560     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPoolin  (None, 248, 256)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 248, 256)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 63488)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8126592   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8250241 (31.47 MB)\n",
      "Trainable params: 8250241 (31.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Input(shape=(data.shape[1], 1)))\n",
    "\n",
    "model2.add(Masking(mask_value=0.0))\n",
    "    \n",
    "# First convolutional layer\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "# Second convolutional layer\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "# Third convolutional layer\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5bd16",
   "metadata": {},
   "source": [
    "# Create merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "074eff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate, Input\n",
    "\n",
    "# Define the input tensors with the correct shapes\n",
    "input_shape_1 = (len(data_train[0]), 1)\n",
    "input_shape_2 = (len(data_deriv_train[0]), 1)\n",
    "\n",
    "print(input_shape_2)\n",
    "\n",
    "input1 = Input(shape=input_shape_1)\n",
    "input2 = Input(shape=input_shape_2)\n",
    "\n",
    "# Get the outputs of the models\n",
    "output1 = model1(input1)\n",
    "output2 = model2(input2)\n",
    "\n",
    "# Concatenate the outputs of the two models\n",
    "merged = Concatenate()([model1.output, model2.output])\n",
    "\n",
    "# Add more layers \n",
    "\n",
    "x = Dense(128, activation='relu')(merged)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = Dense(128, activation='relu')()\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "final_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# # Create the final model\n",
    "from keras.models import Model\n",
    "combined_model = Model(inputs=[model1.input, model2.input], outputs=final_output)\n",
    "combined_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fcbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 03:45:47.559016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-06-07 03:45:48.503542: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560832d2e9e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-07 03:45:48.503619: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-06-07 03:45:48.526060: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-07 03:45:48.690393: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 10s 87ms/step - loss: 0.6892 - accuracy: 0.5610\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 3s 81ms/step - loss: 0.6872 - accuracy: 0.5592\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6860 - accuracy: 0.5614\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6862 - accuracy: 0.5615\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6866 - accuracy: 0.5626\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 0.6864 - accuracy: 0.5626\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6860 - accuracy: 0.5621\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6859 - accuracy: 0.5619\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6858 - accuracy: 0.5617\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6860 - accuracy: 0.5619\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6864 - accuracy: 0.5618\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6852 - accuracy: 0.5621\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6858 - accuracy: 0.5619\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6859 - accuracy: 0.5621\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6859 - accuracy: 0.5619\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6862 - accuracy: 0.5621\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6862 - accuracy: 0.5621\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6862 - accuracy: 0.5621\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6864 - accuracy: 0.5621\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6861 - accuracy: 0.5621\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6865 - accuracy: 0.5621\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6859 - accuracy: 0.5621\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6863 - accuracy: 0.5621\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6861 - accuracy: 0.5621\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6861 - accuracy: 0.5621\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6853 - accuracy: 0.5621\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6852 - accuracy: 0.5621\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6861 - accuracy: 0.5621\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 0.6860 - accuracy: 0.5621\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6853 - accuracy: 0.5621\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6859 - accuracy: 0.5621\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 3s 87ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6859 - accuracy: 0.5621\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6853 - accuracy: 0.5621\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6859 - accuracy: 0.5621\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6858 - accuracy: 0.5621\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6853 - accuracy: 0.5621\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6854 - accuracy: 0.5621\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6860 - accuracy: 0.5619\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 3s 86ms/step - loss: 0.6857 - accuracy: 0.5621\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.6855 - accuracy: 0.5621\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.6856 - accuracy: 0.5621\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 3s 85ms/step - loss: 0.6856 - accuracy: 0.5621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb0ec194340>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit([data_train, data_deriv_train], np.array(labels_train), epochs=100, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a997755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "data_expanded = np.expand_dims(data_val, axis=-1)\n",
    "data_deriv_expanded = np.expand_dims(data_deriv_val, axis=-1)\n",
    "\n",
    "preds4 = combined_model.predict([data_expanded, data_deriv_expanded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77ef252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:00<00:00, 59109.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "class_list = {'0': 'A' , '1': 'B'}\n",
    "\n",
    "num_high = 0\n",
    "num_mis_high = 0\n",
    "num_mis_low = 0\n",
    "\n",
    "tot_exc = 0\n",
    "mis_index_4 = []\n",
    "for i, pred4 in enumerate(tqdm(preds4)):\n",
    "    conf = np.abs(pred4[0]-0.5)+0.5     # pred is just 1 number \n",
    "    pred4_class = int(np.round(pred4[0]))\n",
    "\n",
    "    #print(pred4_class, labels_val[i])\n",
    "\n",
    "    tot_exc += 1\n",
    "    \n",
    "    if conf > 0.9:\n",
    "        #plt.savefig(f'classifications/high_confidence/{ids[i]}.png')\n",
    "        num_high += 1\n",
    "\n",
    "        if class_list[str(pred4_class)] != class_list[str(labels_val[i])]:\n",
    "            mis_index_4.append(i)\n",
    "            num_mis_high += 1\n",
    "            #plt.show()\n",
    "    else:\n",
    "        #plt.savefig(f'classifications/low_confidence/{ids[i]}.png')\n",
    "\n",
    "        if class_list[str(pred4_class)] != class_list[str(labels_val[i])]:\n",
    "            num_mis_low += 1\n",
    "            mis_index_4.append(i)\n",
    "            #plt.show()\n",
    "\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020aa698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6010498687664042\n"
     ]
    }
   ],
   "source": [
    "print(1-len(mis_index_4)/len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1ffac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high confidence classifications = 0\n",
      "Number of low confidence classifications = 381\n",
      "Percentage of high confidence = 0.0\n",
      "Total misclassified = 152\n",
      "Accuracy in total = 0.6010498687664042\n",
      "Accuracy of low confidence = 0.6010498687664042\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of high confidence classifications = {num_high}')\n",
    "print(f'Number of low confidence classifications = {len(preds4) - num_high}')\n",
    "print(f'Percentage of high confidence = {num_high/len(preds4)}')\n",
    "\n",
    "print(f'Total misclassified = {num_mis_high + num_mis_low}')\n",
    "print(f'Accuracy in total = {1-(num_mis_high + num_mis_low)/len(preds4)}')\n",
    "\n",
    "# print(f'Accuracy of high confidence = {1-num_mis_high/num_high}')\n",
    "print(f'Accuracy of low confidence = {1-num_mis_low/(len(preds4) - num_high)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8256c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:00<00:00, 2953.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training for combined model\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "class_list = {'0': 'A' , '1': 'B'}\n",
    "\n",
    "num_high_1 = 0\n",
    "num_mis_high_1 = 0\n",
    "num_mis_low_1 = 0\n",
    "\n",
    "a_mis = 0\n",
    "b_mis = 0\n",
    "correct = 0\n",
    "\n",
    "mis_index_1 = [] # identifier list of misclassified flares\n",
    "\n",
    "tot_exc = 0\n",
    "\n",
    "for i, pred in enumerate(tqdm(preds4)):\n",
    "    conf = np.abs(pred[0]-0.5)+0.5     # pred is just 1 number \n",
    "    pred_class = int(np.round(pred[0]))\n",
    "\n",
    "    print(pred_class, labels_val[i])\n",
    "    \n",
    "    tot_exc += 1\n",
    "\n",
    "    if class_list[str(pred_class)] != class_list[str(labels_val[i])]:\n",
    "        if labels_val[i] == 0:\n",
    "            a_mis += 1\n",
    "        else:\n",
    "            b_mis += 1\n",
    "    else:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd829631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6010498687664042\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(correct/len(preds4))\n",
    "print(1 - a_mis/len(labels_val[labels_val==0]))\n",
    "print(1 - b_mis/len(labels_val[labels_val==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98bcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
